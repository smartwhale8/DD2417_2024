{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478c3b76-d2ed-4b70-8acd-4e40a15ba6a9",
   "metadata": {},
   "source": [
    "# Assignment 4, task 3\n",
    "\n",
    "In this task, you are going to implement a character model based on the Transformer architecture, starting from the provided skeleton. It is useful to first do exercise 2 before starting on this task.\n",
    "\n",
    "The model you are going to implement here will have a context of 32 characters, i.e. it will consider the preceding 32 characters when estimating the probabilities of the possible character coming next. Due to the clever transformer architecture, the model will have less than 50,000 trainable parameters. As a comparison, the simpler model in exercise 2 only had a context of 8 characters but had more than 300,000 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab7eca6-a6b5-4325-be4c-4e0f0ad09e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbde199-fa08-4c83-b496-52c5b69df042",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9510ea6e-4171-41e5-94c7-621d6424042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0 \n",
    "id_to_char.append( PADDING_SYMBOL )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6f3e4-1702-4739-916c-d2e005493697",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.Compared to exercise 2, we will create data points in a slightly different way. \n",
    "\n",
    "The init function reads a training text and splits it up into chunks $n$ characters long. From each chunk, $n$ data points with a corresponding label will be created, as in the following example:\n",
    "\n",
    "Suppose $n=8$. From a chunk $[4,5,9,11,7,7,2,12]$ with 14 being the next character ID, the following data points and labels will be formed (0 is the padding symbol):\n",
    "\n",
    "| Data point | Label |\n",
    "|-----------:|------:|\n",
    "|[4,0,0,0,0,0,0,0] | 5 |\n",
    "|[4,5,0,0,0,0,0,0] | 9 |\n",
    "|[4,5,9,0,0,0,0,0] | 11 |\n",
    "|[4,5,9,11,0,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,0,0,0] | 7 |\n",
    "|[4,5,9,11,7,7,0,0] | 2 |\n",
    "|[4,5,9,11,7,7,2,0] | 12 |\n",
    "|[4,5,9,11,7,7,2,12] | 14 |\n",
    "\n",
    "This way, the model will learn to infer the next character even if the context is shorter than $n$. This is a very useful feature, particularly in 'real' language models, where the known context often is shorter than the maximal context length.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484e5020-20e4-4c9f-8bf1-c97feeae0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset) :\n",
    "\n",
    "    def __init__(self, file_path, n) :\n",
    "        self.datapoints = []\n",
    "        self.labels = []\n",
    "        chars = []\n",
    "        try :\n",
    "            # First read the dataset to find all the unique characters\n",
    "            with open(file_path,'r',encoding='utf-8') as f :\n",
    "                contents = f.read()\n",
    "            # YOUR CODE HERE\n",
    "            for char in contents:\n",
    "                if char not in char_to_id:\n",
    "                    char_to_id[char] = len(id_to_char)\n",
    "                    id_to_char.append(char)\n",
    "                chars.append( char_to_id[char] )\n",
    "            # Then go through all the chars and chunk them up into datapoints\n",
    "            k = 0\n",
    "            while k < len(chars)-n:\n",
    "                for i in range(1, n+1):\n",
    "                    self.datapoints.append([c for c in chars[k:i+k]+[0]*(n-i)])\n",
    "                    self.labels.append(chars[i+k])\n",
    "                k += n\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e657a49b-9609-483a-a205-9533e933de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "dataset = CharDataset('HP_book_1.txt', 32) # Max context is 32 characters long.\n",
    "d63,l63 = dataset[63]\n",
    "d1048575,l1048575 = dataset[1048575]\n",
    "d1048576,l1048576 = dataset[1048576]\n",
    "print(d63[16].item() == l63.item())\n",
    "print(d63[4].item() == l1048575.item())\n",
    "print(sum(d1048576[1:]).item() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3c12c-bc54-45ce-b5f6-cf91816499f8",
   "metadata": {},
   "source": [
    "The __self-attention__ computation is at the core of the Transformer architecture. It is important to get this computation efficient (i.e. vectorized), since it involves many matrix operations that would be very slow if implemented by Python loops.\n",
    "\n",
    "The input to the self-attention computation is a tensor containing a vector for each input token, and the output is a tensor of the same dimensions, containing the contextualized versions of the input tokens (see Lecture 9 and the textbook, chapters 10.1 and 10.2).\n",
    "\n",
    "Your task is to fill in the missing pieces below. Look for \"REPLACE WITH YOUR CODE\" and \"YOUR CODE HERE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2dea08-54af-4ee5-b8b6-a945687dfc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates self-attention according to [Vaswani et al., NeurIPS, 2017]\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, number_of_attention_heads):\n",
    "        super().__init__()\n",
    "        # The number of attention heads cannot be more than the hidden size\n",
    "        assert hidden_size > number_of_attention_heads\n",
    "        \n",
    "        self.number_of_attention_heads = number_of_attention_heads\n",
    "        # Divide the hidden_size roughly equal over the different heads\n",
    "        self.attention_head_size = int(hidden_size / number_of_attention_heads)\n",
    "        self.all_head_size = number_of_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Mapping from input to the query, key, and, value vectors\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size, bias=False)\n",
    "\n",
    "        self.final = nn.Linear(self.all_head_size, hidden_size, bias=False)\n",
    "\n",
    "\n",
    "    def reshape_for_multihead_attention(self, x):\n",
    "        # x has the shape (batch_size, seq_length, hidden_size)\n",
    "        B,S,_ = x.shape\n",
    "\n",
    "        # but we want to split the representation of each token into 'number_of_heads' parts:\n",
    "        x = x.reshape(B,S,self.number_of_attention_heads,self.attention_head_size)\n",
    "\n",
    "        # and treat each part separately. Thus, we need the final tensor to have shape\n",
    "        # (batch_size, number_of_heads, seq_length, attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # All of the tensors below will have the shape (batch_size, seq_length, hidden_size)\n",
    "        query_all_heads = self.query( hidden_states )\n",
    "        key_all_heads = self.key( hidden_states )  \n",
    "        value_all_heads = self.value( hidden_states ) \n",
    "\n",
    "        # All of the tensors below will have the shape (batch_size, number_of_heads, seq_length, attention_head_size) \n",
    "        Q = self.reshape_for_multihead_attention( query_all_heads )\n",
    "        K = self.reshape_for_multihead_attention( key_all_heads )\n",
    "        V = self.reshape_for_multihead_attention( value_all_heads )\n",
    "\n",
    "        # attention_scores will have the shape(batch_size, number_of_heads, seq_length, seq_length)\n",
    "        attention_scores = torch.einsum(\"bhqd ,bhkd ->bhqk\", Q, K) #None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # Scale to reduce variance\n",
    "        # scaled by the sq root of attention head size to prevent extremely large values: \n",
    "        attention_scores = attention_scores / (self.attention_head_size ** 0.5) # None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # Use softmax to turn the attention scores into probabilities.\n",
    "        # We want zero scores to be zero probabilities -- hence we turn\n",
    "        # zero scores into -infinity before the softmax exponentiation.\n",
    "        attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Now produce the contextualized vectors for each head\n",
    "        # The tensor below will have shape (batch_size, number_of_heads, seq_length, head_size)\n",
    "        # Compute the weighted sum of value vectors using the attention weights\n",
    "        self_attention_all_heads_separately = torch.einsum(\"bhqk,bhkd->bhqd\", attention_probs, V) #None # REPLACE WITH YOUR CODE\n",
    "\n",
    "        # For each token, we now want to bring together the representation coming from each head.\n",
    "        # The 'self_attention' tensor below should have the shape \n",
    "        # (batch size, seq_length_, self.all_heads_size)\n",
    "        # Concatenate the heads and reshape to the original hidden size\n",
    "\n",
    "        self_attention = self_attention_all_heads_separately.permute(0, 2, 1, 3).contiguous() # None # REPLACE WITH YOUR CODE\n",
    "        self_attention = self_attention.reshape(hidden_states.shape[0], hidden_states.shape[1], self.all_head_size)\n",
    "\n",
    "        # Finally, make sure that the output has the correct dimensions (batch_size,seq_length,hidden_size)\n",
    "        return self.final( self_attention )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feb356-82ee-4370-99de-23dc153ddc62",
   "metadata": {},
   "source": [
    "After the self-attention computation, the Transformer encoder block contains layer-normalization computations and a feed-forward layer. The code is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf90f50c-8485-4e83-a7b4-670c643b5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    The position-wise FFN that follows after the self-attention\n",
    "    computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        for module in (self.fc1, self.fc2):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block.\n",
    "    \n",
    "    This version differs from the original version in  [Vaswani et al. NeurIPS 2017],\n",
    "    and applies the LayerNorm before the self-attention, and before the FFN, as this\n",
    "    has proved to be beneficial (see [Nguyen and Salazar 2019]).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, number_of_attention_heads, dropout_prob) :\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(hidden_size, number_of_attention_heads)\n",
    "        self.ffn = PositionwiseFFN(hidden_size, dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln1(x)\n",
    "        x2 = x + self.dropout(self.attn(x1))\n",
    "        x3 = self.ln2(x2)\n",
    "        x4 = x2 + self.dropout(self.ffn(x3))\n",
    "        return x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edefcae-564e-4a79-8724-42f7f218f4e7",
   "metadata": {},
   "source": [
    "Here is the actual character-based language model, which uses the Transformer implementation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c540c7d3-04e8-4ccd-907c-75b7b3d999e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hyper-parameters for training ============== #\n",
    "\n",
    "class Config :\n",
    "    number_of_transformer_encoders = 1\n",
    "    number_of_attention_heads = 1\n",
    "    hidden_size = 64\n",
    "    dropout_prob = 0.1\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0003\n",
    "    weight_decay = 0.000001\n",
    "    no_of_epochs = 100\n",
    "\n",
    "MAXLEN = 32   # This is the number of characters we will consider when \n",
    "              # predicting the next character\n",
    "\n",
    "# ======================= The model ======================= #\n",
    "\n",
    "class CharLM(nn.Module) :\n",
    "\n",
    "    def __init__(self, config, no_of_input_chars ) :\n",
    "        super(CharLM, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(no_of_input_chars,config.hidden_size)\n",
    "        # Make sure that the padding symbol (which has ID 0) is embedded\n",
    "        # as a vector of 0s.\n",
    "        self.embed.weight.data[0].fill_(0)\n",
    "        self.positional = nn.Parameter(torch.randn(1, MAXLEN, config.hidden_size))\n",
    "        modules = [EncoderBlock(config.hidden_size, \n",
    "                                config.number_of_attention_heads,\n",
    "                                config.dropout_prob) for _ in range(config.number_of_transformer_encoders)]\n",
    "        self.transformers = nn.ModuleList(modules)\n",
    "        self.final = nn.Linear(config.hidden_size*MAXLEN, no_of_input_chars)\n",
    "\n",
    "    def forward(self,x) :\n",
    "        number_of_datapoints = x.shape[0]\n",
    "        # First create a mask distinguishing 0 from positive word IDs\n",
    "        non_zero_mask = (x != 0)\n",
    "        word_embeddings = self.embed(x)\n",
    "        # Add positional vectors in all non-padded positions\n",
    "        pos = self.positional.expand_as(word_embeddings)\n",
    "        pos = pos * non_zero_mask.unsqueeze(-1).float()\n",
    "        t = word_embeddings + pos\n",
    "        # Then apply the transformers and make a final prediction at the end\n",
    "        for transf in self.transformers :\n",
    "            t = transf(t)\n",
    "        flattened_transf = t.reshape(number_of_datapoints,1,-1)\n",
    "        result =  self.final(torch.tanh(flattened_transf))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe7817d-f56f-465c-8499-aacc4298889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "There are 442720 datapoints and 81 unique characters in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:03:00 Training starts\n",
      "16:03:16 End of epoch 1 , loss= 2.6432247161865234\n",
      "  ann en ea nt eane  aartyye  aarny ea na nt ea nt ea ta taeny  aonn  aanng yt aonn  aanl got aonn  atne ya  aann gaen  taon  taonn the ee rayry ya ro laenr yae  ooon no tthee  aarn garny  toou nte  aont  aann yo ta nt ea na ttnee  \n",
      "\n",
      "\n",
      "\n",
      "\"\"\n",
      "\n",
      "\n",
      "\"\"\"\" \n",
      "\n",
      "\"\"\"\"I\n",
      " \"a\"rsrryy, \"\" \n",
      "\n",
      "\"\n",
      "\n",
      "\"\"\"\"\n",
      "\n",
      "\"\"\"\" \n",
      "\n",
      "\"\n",
      "\"\"I\"\" ,\"\"P\"\n",
      "16:03:32 End of epoch 2 , loss= 2.373910665512085\n",
      " se anne ya nagre yasn gatl aon goon to to unt hae  o\n",
      "uen ta nte rte yo uss atle roy efn earny  atne roy enre do uonn toh eon tu theer yas  aann esr yasd inn eard yo lulne  aonng earry yo kann goo nne  tohe aund  waan to thee  aann gaann got  aonng et oon toh eo kn eg ann toh eo st ean toh en es aar\n",
      "16:03:48 End of epoch 3 , loss= 2.1999621391296387\n",
      " and on to waan to the ya sat hem inn to to won te an toh eg and our yo ung att hee rom oun to ungd to the asl ean to tohe  waan toh en thee  aasn toh ean do ung en tohe and en to wan too wna the and inng toh en es at ooun tohe and  aon to to un tohe  \n",
      "a\n",
      " \"osu toh me and int hae no ut hainn gat hoin\n",
      "16:04:04 End of epoch 4 , loss= 2.1780648231506348\n",
      " sened an to to wher you pas toin to fne and  Harryy  atlee and en tooun to ung et hom uon the as tooung the and et hom ing as tooun to the anl atl atl yo four to ung ant hom anit ham anre toh te and en thee rame sat home loon the rgoo the an tol and ing anl yo fur mout hom uon to the and en thee ro\n",
      "16:04:20 End of epoch 5 , loss= 2.3076953887939453\n",
      " sead ed an to thee to toh en to to the and en thee rame sat home to foun toh en thee ras to thee rame sparin catl youn tohe as to tohe to the as to toh eg and an thee rom ande  \n",
      "aI to tohee the and an thee ray so tohee wan to to whe asd en toin to fa ballel your toh mou thhe as to thee rom unpe and\n",
      "16:04:36 End of epoch 6 , loss= 2.1344213485717773\n",
      " sead aly an ter and ange toull oun the as toul to fole the and and and anged and ang and any oul to ugt he asine the as ped and anged and any toul to fuled ang anind an Ron ung anind an to the and one the as tore the poured and any to fuled ange the as the as asind and and any ungt hem and and and \n",
      "16:04:52 End of epoch 7 , loss= 2.0054426193237305\n",
      " sead and y and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "16:05:09 End of epoch 8 , loss= 2.0293664932250977\n",
      " sead and to furne just and and the and and and the and the and and and tore and the was and the the sape and and the came and and the and ofe the and and the peare and and to the and and and to furne jume and to the and and and and and the and and the and and the and and the and and the and and the\n",
      "16:05:25 End of epoch 9 , loss= 2.024259567260742\n",
      " seame and the calle the Vored ofing and and and the and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and \n",
      "16:05:41 End of epoch 10 , loss= 1.9254707098007202\n",
      " sead and the and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and\n",
      "16:05:57 End of epoch 11 , loss= 2.0962092876434326\n",
      " seard and the fore and and to the and he and and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and and and and and and and to and an\n",
      "16:06:13 End of epoch 12 , loss= 2.0261423587799072\n",
      " seard and the calle as and the was quille are for the forme forme the said Harry, and the was he all as and and the way seame and the was and and and the and to and and and and and and and to and and and and to and and and and to and and and and to and and and and to and and and and to and and and \n",
      "16:06:29 End of epoch 13 , loss= 2.0219407081604004\n",
      " seard and the fore and and the came and the cat quind Hermione a staid for and the came and the came as fore the for and and the came and and and the got all as and to and and the and and and and the all offing and they and a stake was and the come and the was quind and owere and was and and and an\n",
      "16:06:44 End of epoch 14 , loss= 1.9200977087020874\n",
      " seat all and the was and and and they and and the can't any ase as and the come and and the was and and and they and and the can't any ase as and the come and and the was and and and they and and the can't any ase as and the come and and the was and and and they and and the can't any ase as and the\n",
      "16:07:00 End of epoch 15 , loss= 1.8574714660644531\n",
      " seather.\" \n",
      "\n",
      "\"It's fore just all of the pursting and and and to the pot and and and and and and and and the got all at and the staid Harry fore forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme forme \n",
      "16:07:16 End of epoch 16 , loss= 1.951723337173462\n",
      " seard and the can fore and the could and and and and and and were and and and and one the got at and and and and and want was and the could and and and and and and were and and and and one the got at and and and and and want was and the could and and and and and and were and and and and one the got\n",
      "16:07:32 End of epoch 17 , loss= 1.8088167905807495\n",
      " seat and Ron, and Harry gowere staked and they and and the was and the poked and and and and and and they and and and and offind the poked and and and and and and they and and and and offind the poked and and and and and and they and and and and offind the poked and and and and and and they and and\n",
      "16:07:48 End of epoch 18 , loss= 1.8643412590026855\n",
      " a staid Harry fund the was and and the poing and and and and and and and the got all at and and and and and walke was and and and and and and the got all at and and and and and walke was and and and and and and the got all at and and and and and walke was and and and and and and the got all at and \n",
      "16:08:03 End of epoch 19 , loss= 1.8655846118927002\n",
      " he spece and and and and and and the grought the pace as and the said Harry forther and Ron, and the staid Ron and for the came was and the said at the fort all as forme -- the could and and and yought he was and and and and and was and the was quicke and they and was and and was leard at the forge\n",
      "16:08:19 End of epoch 20 , loss= 1.9211554527282715\n",
      " the said and and and and the poke and and and a forry gaid Harry for the packed and and and the the pain, and as and the poke and Harry forged and and was Muggle all as and and and and and a flooked and and the was and and there and was and the and and and was and and there and was and the and and \n",
      "16:08:35 End of epoch 21 , loss= 1.8942155838012695\n",
      " a sme and the was and the was be the for painted and see to and Gorme was and the poing to faid Hermione and ther staing to the pain and was and and the see and and there and was and the was and the was and and the see and and and and the see all as and the staing and the poine the poine a quite th\n",
      "16:08:51 End of epoch 22 , loss= 1.7957754135131836\n",
      " Hermione was was the the seard and the poing and and and and all of the see and and and and and all off the forme form. \n",
      "\n",
      "\"Yeas and and the staid the pole as and and the came and and was leard all of the fir and and said and to flooked the got the the form. \n",
      "\n",
      "\"It have and Harry form just all of the\n",
      "16:09:07 End of epoch 23 , loss= 1.78547203540802\n",
      " a sme and the and and and and said Hermione and the was and and there and was and the and and and was and and the gotter and and was and and the staing and the was and and and and to and he a squitent of the pace and and and all one the surmione a forre stain.\" \n",
      "\n",
      "\"Yes, and I've gol as and and and a\n",
      "16:09:23 End of epoch 24 , loss= 1.7587510347366333\n",
      " a sme and the was and the poing and and and was and staid Harry, was the sleather and they see as and the poing and the poing the poing to stail for Mr. Potter poing to the Gryffindor McGonagall ather and and and what were as and ther staing to Mr. Potter Muggle on Mr. Noom Mugle was ound a staid w\n",
      "16:09:39 End of epoch 25 , loss= 1.7779470682144165\n",
      " a sme and the poine the pull as and to a said Harry quither and was and the pull of the was and them and a said and ther be and and was and the poing to at for them Muggle was and and and a was the pull of the poing the pointion. He was and Ron, and were a suldn't all of the poing to for all at he \n",
      "16:09:55 End of epoch 26 , loss= 1.8471046686172485\n",
      " a sme and the formione was the the Snape and and a form and there was and the and and and was were was up and the form and to form and ther for and to the patter as and the was and the was and and was leather and they and and a forre staing the pust and Ron. \n",
      "\n",
      "\"Yevery, any see a seath and see a and\n",
      "16:10:11 End of epoch 27 , loss= 1.761133074760437\n",
      " a smether and and there forre still of the pair and and and and to the pack and and there and they and and and and all of the pace and and and was to and they and and was and and to the poing to face and and and to the surped and and to and a Ron, and Hermione was was to the said and and were Mr. P\n",
      "16:10:27 End of epoch 28 , loss= 1.8958734273910522\n",
      " a small of the pace and they and a squiter and and the looked a forry for them. \n",
      "\n",
      "\"It's said the was and you was und the was but and the sudder and and the call of the form of the pace as and them of the sumper and and to the sture and all of the for Mugleme out and the was and and the said Ron, an\n",
      "16:10:42 End of epoch 29 , loss= 1.8936947584152222\n",
      " Hermione a said Ron. \n",
      "\n",
      "\"It'm \n",
      "\n",
      "\"It said Harry, was Mugle was a just at the pulled and a form and ther form and there stain. \n",
      "\n",
      "\"He caus and arought the Greather, and the staing and the pulled and a form and Ron the pulled and a the forre puse and and a forry form just the was and the the pace and an\n",
      "16:10:58 End of epoch 30 , loss= 1.8086358308792114\n",
      " Hermione a supped the was and to a stain, Harry quitent the was and see a quicked and of the poing to face the pain and Harry for the pure and and the pulled and a form and for the pointione a squicked and and the said Harry quile,\" said Harry, squitilly.\" \n",
      "\n",
      "\"It was Ron, and Harry for purst them an\n",
      "16:11:14 End of epoch 31 , loss= 1.901463270187378\n",
      " and and were and was and the was the sture and to the pace and they and and were and a the pulled at the sto a said Harry for the packed and and the said and was was be the got the Gryffing to from Mr. Pottery just of the was Mugle as and any was up the got Mugght as a said Ron, and Hermione just t\n",
      "16:11:30 End of epoch 32 , loss= 1.7749087810516357\n",
      " a the staid and the forre forgether, and the grower and the was and the was said Hermione a squet all and to the said and was to the staid and was leather and see the were as and there and the forge staing to back and to a will and they and were a sudder and Hermione were the grought the pain a sai\n",
      "16:11:46 End of epoch 33 , loss= 1.7469271421432495\n",
      " a the a said Harry quicked and of the pust all and there and and were and and the come and and the care and the case and and the can to at looke the got at the for as and the pull and they as and the poing to at the said Harry form and the summert all of they the for pain and the purst and Gorder a\n",
      "16:12:02 End of epoch 34 , loss= 1.8902361392974854\n",
      " a staily. \n",
      "\n",
      "\"It'm said the were sturpled as and the pull as and they and and a from and the sunt and the came and and and was leather and ther and Ron. \n",
      "\n",
      "\"Yes and the said a for the for them and there and and was the still as and the and them. \n",
      "\n",
      "\"They said squile and they and a said Harry quicked a\n",
      "16:12:18 End of epoch 35 , loss= 1.8527014255523682\n",
      " a smaking for the the pain and the pointed and Hermione a squile the past and to all as for the found the pace and and and were and and ther be the got the pall as and to a said Hermione said Ron, put was the to a sume and the came and was the staid for and to and the see and and was the and the se\n",
      "16:12:34 End of epoch 36 , loss= 1.8747838735580444\n",
      " and Hermione a sunt the pointed a forge to for the pace of the past as and the some and Gryffindor.\" \n",
      "\n",
      "\"It's said Hermione a suddent a looked and the was and the was a said the pull and they as and the pointed a face staily for -- and the was Muggle as and a squile and to a said the for the pair an\n",
      "16:12:50 End of epoch 37 , loss= 1.828112006187439\n",
      " and Hermione and Hermione was and the the got the pair and and to the got the pair and to for staily. \n",
      "\n",
      "\"But said and Ron and the was because and the and the see and and and there and they as and and the see and and and there for the forgetter as and Ron. \n",
      "\n",
      "\"Now were stay see and the study,\" said H\n",
      "16:13:06 End of epoch 38 , loss= 1.8632242679595947\n",
      " and Hermione a squicked a sleather and the was Muggle as and Coloking and to the see a said quile they and the poing they seard all of the staily. \"The said Hermione and Harry was was the sturned and and were the got the and the said and and the was leas; and the said Hermione and ther staily. \n",
      "\n",
      "\"I\n",
      "16:13:22 End of epoch 39 , loss= 1.9184187650680542\n",
      " and said a the pull and the was and the was and to staily. \n",
      "\n",
      "\"It's said Hermione a sto the pather forging to the play and the said and Harry was was the staing the pair forgetter.\" \n",
      "\n",
      "Hermione and Harry, where just as the pull as and the going the past the was and and there was and the looking and t\n",
      "16:13:38 End of epoch 40 , loss= 1.772155523300171\n",
      " and started and was the sturple as and to the pointly as for the said and to the said and see and the poing the past the see and and there forgething to back and and said Harry sudled to the sturply as for them of the see a said and to the said and the can and the was and the pull of the point the \n",
      "16:13:54 End of epoch 41 , loss= 1.8388707637786865\n",
      " and studley and the stude and and and the said and Hermione and the sle as and the to all as and the sturple and they as and the was quicke as as all forgething for the stain, they was staily. But was the they see as and and the looke as and the was quile as and and the starter and the see all as a\n",
      "16:14:09 End of epoch 42 , loss= 1.7994227409362793\n",
      " and and and the were and as and to the pointione a squicked they and as and the pull as and to the pointione a squicked they and as and the pull as and to the pointione a squicked they and as and the pull as and to the pointione a squicked they and as and the pull as and to the pointione a squicked\n",
      "16:14:25 End of epoch 43 , loss= 1.7658849954605103\n",
      " and Ron and the poing to a said the pain. \n",
      "\n",
      "\"He said Ron, and Harry and Ron town the and the see a said and to the poing to a said the poing to the play and and some a staid the poing the pust as and Ron. \n",
      "\n",
      "\"Yes, said Harry, who sturned a face the pointed a for the painted and the she staing and th\n",
      "16:14:41 End of epoch 44 , loss= 1.8373363018035889\n",
      " and staid for the pain. \n",
      "\n",
      "\"They said sayid Hermione staid Ron, the poing to a said Harry quicked and said Harry, who the pull of the see and a for the from and and the see and and the pointed a forge to from form any, cain Harry was still and the still as for they pair for and them and a sto all fo\n",
      "16:14:57 End of epoch 45 , loss= 1.7590407133102417\n",
      " and a slee and and the still as and the staing the points; the said Harry, squitent the can the past and and the see all and the said a squile and the and a squicked and and the stime and a suddent the pointion the still and the see a squile and of the for and they and a suddent the just as still a\n",
      "16:15:13 End of epoch 46 , loss= 1.6789493560791016\n",
      " and started and the see all and the pain a squile and of the for them. \n",
      "\n",
      "\"It's said anyther, Mr. Potter, Mr. Were and was and and the was and the was and and the see and and there and the for the paint the pain. \n",
      "\n",
      "\"Pevery, sound on the purpled arought the the see and and and the points; the was be \n",
      "16:15:29 End of epoch 47 , loss= 1.793752670288086\n",
      " the sumped and a quite the point they for staing the pain.\" \n",
      "\n",
      "\"Yes, Mr. Potter you and the smether said and a quicke and of the forgething ford the points and the stoper all and them as and and a quicke a said they for and the can the staing to the poing to a said the pust and Harry for the fought \n",
      "16:15:45 End of epoch 48 , loss= 1.765254259109497\n",
      " and suddent all and the stude and and a forgething to be the forget. \"Look!\" Harry student, you a Ron the still of the poing put a fould the gave of the forgething Gryffindor and said Gryffindor and the pust and and the see and and and the student and of the for and the see said and the come and an\n",
      "16:16:00 End of epoch 49 , loss= 1.7987425327301025\n",
      " and a sleeped at all and the starter and to a the going and Harry for points; the Greath, Mr. Nevery of the see all weard on and the Gryffindor and said all and the sturned a for the for Mr. Were was and the purpled and the see all and the painted and the see all and the see all and the poing the p\n",
      "16:16:16 End of epoch 50 , loss= 1.60459303855896\n",
      " and said Mr. Goy, who the staing the past and the puse and the got all and the see and and and the were and and the got be and the see and and and the staid and the could at all and the started and and the still and the still of the past all and the pull of the purpled them. \n",
      "\n",
      "\"We sayed and a looke\n",
      "16:18:08 End of epoch 57 , loss= 1.8828380107879639\n",
      " and and the was be and the the see and and one a four and the the be and and the the see and and the pointione a squile and the for all of the poing the pair and the pused and the pointione the see and the pain and and the see and and the pointione a squile and the for all of the poing the pair and\n",
      "16:18:24 End of epoch 58 , loss= 1.775079369544983\n",
      " and a said the sturned and the points; and Hermione was a staily the just a said and the the pain a said and the points; the were and there was the got the pain the was Muggle and Harry, just were for the caught the got the past all and the see and a four and the the was and the and the see and and\n",
      "16:18:40 End of epoch 59 , loss= 1.840571403503418\n",
      " and a sleep all and the staing to back to stain Harry was staing to the still of the past all and the pointed and said Harry for the sturned and the pointed and were be they got the Greath see and and and the was staing to the still of the past all and the pointed and said Harry for the sturned and\n",
      "16:18:56 End of epoch 60 , loss= 1.7356629371643066\n",
      " and staid for the pain. \n",
      "\n",
      "\"They see as quickled as of the stay and the were and a found the poing the pain the still anything and and and and the see and the see of the just and the could at all and the see and and and the see and and the see and the was and the was and said of the staing the could\n",
      "16:19:11 End of epoch 61 , loss= 1.8232109546661377\n",
      " and a said and the staing to the staing to the pain the poing the past anything and said and the staing the said and the poing the pair they for the pair the pather said and the come and and the was and and and one the was and and the see and and the see and the points; and the pointed a for the pa\n",
      "16:19:27 End of epoch 62 , loss= 1.8746944665908813\n",
      " and suddenly arm and the going the poing the past as and the sup the and the see and the pointed a for the pain for the still for the see of the poing the said the pulled a said and the poing for the see pair for the sture for the for the come and the see all and the see your and the staid Ron puse\n",
      "16:19:43 End of epoch 63 , loss= 1.7742787599563599\n",
      " and a smele and of the stain the stay and and the staily and the stain the see and the pointione the stoped the can Vernon, Slything Gryffindor the first pain for the staing the come of the sup of the firstley.\" \n",
      "\n",
      "\"Well -- and was a funning and the stoped and a fought the firstand the staing the pa\n",
      "16:19:59 End of epoch 64 , loss= 1.7781285047531128\n",
      " and staid the Gryffindor the see and the points; they were pust and the said and Gryffindor the see was the points; they were puseled and But was they said and the Gryffindor a staily. \n",
      "\n",
      "\"It's see and the was Muggle a and a four the Gryffindor for forgething to staing the just as as and the pull of\n",
      "16:20:14 End of epoch 65 , loss= 1.7502774000167847\n",
      " and staily for the see were Vernon, \"Well of Professor.\" \n",
      "\n",
      "\"It said Ron.\" \n",
      "\n",
      "\"Profess -- can's and the suse all of the Griffore and and the see a fould and the suddent a points all fooking and the see a the puse and and the suddent all and of the see a four and the going the pasted all and the sturn\n",
      "16:20:30 End of epoch 66 , loss= 1.8398969173431396\n",
      " the see all and the see a fould and the sumped and and the started and said a staily. \"Yes, Ron, whink eas and the supped at anything and said and the points; the see can the past all and the see all and and the see and a fould the going the poing they and a said and said a looking the said the Gre\n",
      "16:20:46 End of epoch 67 , loss= 1.8291465044021606\n",
      " and a sturned at the can's and the sture and the Gryffindor the see and the pain and the stoped and the puffindor for the sture pain for the sturned and the Gring to the was and the pointed and was staing to the still as and the points. He was see and the pull around the Gryffindor for see sture po\n",
      "16:21:02 End of epoch 68 , loss= 1.7930166721343994\n",
      " and stilled are were of the points; the pointer for forgething to the Look and the was all as sture all of the forgething ford and see the see and the see to the points; the Bording the pain and the pull of the sure all of the poing the past all and the see all of the poing the past and and all of \n",
      "16:21:17 End of epoch 69 , loss= 1.811846137046814\n",
      " the see all and the see are was and the to the pointed as and the pulled and a quickled as of the fould for and the pulled and and the be of the pointer and the past all and the sumed and a forge the got the pastione a stood the put Muggle and to the pointe a said the see all and the see are was an\n",
      "16:21:33 End of epoch 70 , loss= 1.7549593448638916\n",
      " and a staily. \n",
      "\n",
      "\"You said the staily. \"Well of sumped and the pulled a fould for points; the forgething and the see packed as and at the the can and and the see and a fould the going to a said the the past as and the couldn't all of the points; they was and the pulled and a said the Gryffindor for \n",
      "16:21:48 End of epoch 71 , loss= 1.7829869985580444\n",
      " and staily and the see and the points; they were was and to the purple and to the poing the past as and to the sturned a forgetter and the Gryffindor and the could of the poing the past as and all of the sudden the was to a said the pull of the was and the purpled and was and the was and you the th\n",
      "16:22:04 End of epoch 72 , loss= 1.8779470920562744\n",
      " and stilled a fould be the the pointed and Harry for and the points; the was they as quickle and undred all and the see were to the pointed a the past all and a quille of the stain, the said the said Harry, where Ron purpled and the stoped at the the pointed a four for the forgething to peather Mug\n",
      "16:22:19 End of epoch 73 , loss= 1.7865195274353027\n",
      " and staying and the still at as was and the side and the platful and the see to first; to Hermione Weasley, Voldemort was a said Weasley.\" \n",
      "\n",
      "\"You said Profess, Muggling Quirrell and sture and Harry, said and the pused at all food and with a stoped the just a face put the said and all and the see a \n",
      "16:22:35 End of epoch 74 , loss= 1.8081822395324707\n",
      " and staying the the said a four the Gryffindor for forgething to still and the was and the sumped and the suddent as a was and the sturned and the see and the points; they were pain for the come and Cormione of the supped to and the points and for points; the was the the Sould and the points and a \n",
      "16:22:50 End of epoch 75 , loss= 1.867400050163269\n",
      " and staily and the see were they was mord the see and the pain and the pointer and the Gryffindor and sile a quickled and the see quickle and of the pointed and the Gryffindor and said all the just all and the see all of the pointed and the Gryffindor and said all the just all and the see all of th\n",
      "16:23:06 End of epoch 76 , loss= 1.9226466417312622\n",
      " and suddenly and the staily. \"Well of the see pather as the sumped as and the poing the past as and the sumped and a fought of the said all and the staily. \n",
      "\n",
      "\"Mrs. Nevery, said and was stude a a four the Gryffindor for the couldn't just all of the see pair the suddenly and was and there for the sai\n",
      "16:23:22 End of epoch 77 , loss= 1.824677586555481\n",
      " and all around the sudden't all over of the going the play. \n",
      "\n",
      "\"The Boy, squieth he said all as and the and the was and the still of the see all and a four and the the to still of the Greyfing and and the see and at quickle and the was the the Slythering to a squile and of the for the just all are t\n",
      "16:23:37 End of epoch 78 , loss= 1.8138773441314697\n",
      " the and the pull of the was supped and to the pather and and the said and the platfering to for pointed and the case and But the caster; they was and the was and the was supped and the the see and and one points; the was of the for the past all and the seemed and the poing the said the said and the\n",
      "16:23:53 End of epoch 79 , loss= 1.7745264768600464\n",
      " the see all and the see and the points; they were put of the sudden and was they as and the sudden anything and the see and the see and the points; they were put of the sudden and was they as and the sudden anything and the see and the see and the points; they were put of the sudden and was they as\n",
      "16:24:09 End of epoch 80 , loss= 1.9266955852508545\n",
      " and suddents. \n",
      "\n",
      "\"Not will of the poing them a said and the couldn't be and the points; they were and Unce Vernon, on Muggle was mortaing the the past all and the see stain, Gryfing for the for them as he suddent and we and the said and said the staily. \n",
      "\n",
      "\"She see was mouth and see and at all and sa\n",
      "16:24:24 End of epoch 81 , loss= 1.8429396152496338\n",
      " and still ather was still and the were Vermione the stopped as and the to all around the still for the for forgething to pastione the stain the pointed and the see pointer the stopped the pastione the pointer and Goy, \"Well armione the studer the for the forgetter pacted and the pointed a for Fluff\n",
      "16:24:40 End of epoch 82 , loss= 1.7599880695343018\n",
      " and stay.\" \n",
      "\n",
      "\"He said said the sturned and the cate and the pointer and the Gryffindor and said Hermione just the Gryffindor the forgething to purpped and they was and the stand the pointed and the see of the just of the pather. \"They see were you the said the Gryffindor -- squiled and the was be a\n",
      "16:24:56 End of epoch 83 , loss= 1.8812239170074463\n",
      " and stance and the see the the were to and the points; the Bance But Muggled Potter Finning town the see of the pointed to see the pointed a for the pointed and But was they were they see they were they pasted as and the for the sumped and to the pointed a for the past all around the staily and the\n",
      "16:25:12 End of epoch 84 , loss= 1.7383111715316772\n",
      " and stay Mr. Weasley. \n",
      "\n",
      "\"So the purpled and Professor Mr. Goy, wording the points anything to staily. \"He said said and you fall and the the see and and the points; and Bothering the points and and the points; the was they as and the were and you the was the suddenly all of the points. He pulled an\n",
      "16:25:27 End of epoch 85 , loss= 1.842208743095398\n",
      " the seemed and a quickle and of the points; they were the pointly and suddenly and a four the for the points; the Bandrer they can anything the stain for the for the pointed and the pack and a suddenly. \"The said Ron. \n",
      "\n",
      "\"It's the see quille just of a suddenly.\" \n",
      "\n",
      "\"She was sudden the was a suddenly \n",
      "16:25:42 End of epoch 86 , loss= 1.8066673278808594\n",
      " and and the puffindor for for for for for for forgething to pointed and the put the case and the see and and the pull and all around the pointed and the see and and the see and the pointed and the Gryfing and the see as and and the suddenly all was and the see and the were Vernon, once, on Profess,\n",
      "16:25:58 End of epoch 87 , loss= 1.8103398084640503\n",
      " and staily, but he see we to give on the started as and the pointed a for Every staing and the come and a said the said at the suddenly all armione the see and the was they as and the see and and the points; the was see as quickle a sto the was all as quiled the was all at going one the was and a s\n",
      "16:26:14 End of epoch 88 , loss= 1.662811040878296\n",
      " and started and the the said as and the points; and were Ron. \n",
      "\n",
      "\"They seemed as they said of the started and the see and and the seemed and a from the forgething to peached and Band the was sile the said the started and the points and a quickled and and the the see and the points; they were put of \n",
      "16:26:29 End of epoch 89 , loss= 1.8194408416748047\n",
      " and and the looked a the stain, they were squeter the of the going to pain, they were they see points; the Band and was the sumped and the couldn't all of the points; the seemed and a four for for the forgething for the for for the for the just the pointed a fould be and the points; they were point\n",
      "16:26:45 End of epoch 90 , loss= 1.7587740421295166\n",
      " and and the pointed and supped and the points; they were points and suddenly around the Gryffindor the see and the pair and the come of the pointed them. \n",
      "\n",
      "\"We said Harry, leave the was a still of the pointed and were and the can Vernon, I was he see to the points; Mr. Ben quiled and was was the an\n",
      "16:27:00 End of epoch 91 , loss= 1.7595208883285522\n",
      " any stance and the couldn't all of the points; they were and of the points; they were the see and But was they were the see the points; they were was and the points and for the come of the see to the patch of the poing pain, Harry started the was; Hermione the staily. \n",
      "\n",
      "\"It's see any,\" said Harry, \n",
      "16:27:16 End of epoch 92 , loss= 1.809902548789978\n",
      " and suddenly armione the going the pointer and Ron. \n",
      "\n",
      "\"He said was quickle arry, said the starter and to staing Mr. \n",
      "\n",
      "\"It see said Harry, squiled them and said Mr. Nowere you we and the and the points; they were they see are the points and for the come of the sumped and the points; they were of the\n",
      "16:27:32 End of epoch 93 , loss= 1.7393633127212524\n",
      " and staying the the said a said the Gryffindor for said for the just all around the for the points; the Bast all Bans they were was and the the was as a stain, they were was and the points; the was was and the was and and the for the for the said the see to the was the was and and the pull and a fo\n",
      "16:27:47 End of epoch 94 , loss= 1.9192458391189575\n",
      " and stay Mr. Weasley, and Band Ron was quilled a the still of the pointed the points and form of the supped and the said and where staing to be they were and the case and there for the for the couldn't he seemed at for the for them and to the points; they were of the seemed and a quickled and as al\n",
      "16:28:03 End of epoch 95 , loss= 1.8301677703857422\n",
      " the and the pointer the said a still and the see was and the points; the was they see a suddenly. \n",
      "\n",
      "\"It's see starter and the was said Harry, squilting the was the pointer and the points; they were to the seemed and a sup the the said as and the points; the was they see a suddenly. \n",
      "\n",
      "\"It's see star\n",
      "16:28:18 End of epoch 96 , loss= 1.705337643623352\n",
      " and and the was the see and the was they as and the was quile and the said and were Vermione, where said the points; Hermione and the can anything the said and the pointer the said and the come of the sure and the got Mr. Potter. Nothering to staing the cared and the points; the was the see was jus\n",
      "16:28:34 End of epoch 97 , loss= 1.7855230569839478\n",
      " and and the was the see and the was they and the points and and the started and the said and Gryfing and the see the was and you a suddenly and the started and the the seemed and the see and and the points and a looking the said the Greath and and suddenly and a suddenly. \n",
      "\n",
      "\"It's see still for they\n",
      "16:28:49 End of epoch 98 , loss= 1.9228057861328125\n",
      " and a supped at the the forgethering to for the past all to the was me the see they as and the points; and Bans in for the points; they were to the pointed a pain for the for the said the pointing the points; Muggled and the pastion and the said as and the sudden and the put a face and the got the \n",
      "16:29:05 End of epoch 99 , loss= 1.9044580459594727\n",
      " and and the pointer the said a the pointed and the see and and the points anything to still and the was said a said the the the see were the just a and and the pointed a forgething to the points; Hermione Weasley. \"Well are was up and just all around the the said a the said the Gryffindor and said \n",
      "16:29:21 End of epoch 100 , loss= 1.7138264179229736\n",
      " and staying the the said a said the Gryffindor and said Ron. \n",
      "\n",
      "\"And But Norried as face the suddenly almose and the points; the see of the points. He fould and there was of the Gryffindor for the for the forge the points and along at all of the Gryffindor and sile and there was Muggle of the the po\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "config = Config()\n",
    "training_dataset = CharDataset('HP_book_1.txt', 32)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "training_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "\n",
    "charlm = CharLM( config, len(id_to_char)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam( charlm.parameters(), lr=config.learning_rate )\n",
    "\n",
    "charlm.train()\n",
    "print( datetime.now().strftime(\"%X\"), \"Training starts\" )\n",
    "for epoch in range(config.no_of_epochs) :\n",
    "    iteration = 0\n",
    "    for input_tensor, label in training_loader :\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "        iteration += 1\n",
    "\n",
    "    print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "    charlm.eval()\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        char_list = list(\"he took out his wand and\"[-MAXLEN:])\n",
    "        for i in range(300) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\n",
    "    charlm.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9b627-90e7-440e-8dae-920030e9b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  am I supposed to do something here?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \n",
      "\n",
      "\"Yes, Mugg! He allked as and sudden the the as\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Yo give me some cool stuff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind- they and pointed and squickly the see saying \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  i have no idea what you are saying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to the squind the pointing as for the stured as a\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  is this muggle talk?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \n",
      "\n",
      "\"Ron!\" said Hermione said as for the the could\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  are you Snape?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \n",
      "\n",
      "\"My same Mr. No,\" said Ron,\" said Hermione sud\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  yeah sure, I am not buying that\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Not was Not alme and just as fame forgething \n"
     ]
    }
   ],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "while True:\n",
    "    text = input(\"> \").strip()\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    char_list = list(text[-MAXLEN:])\n",
    "    # Generate 50 characters starting from the input text\n",
    "    try :\n",
    "        for i in range(50) :\n",
    "            input_tensor = torch.tensor( [char_to_id[c] for c in char_list] + [char_to_id[PADDING_SYMBOL]]*(MAXLEN-len(char_list))).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character = id_to_char[new_character_tensor.detach().item()]\n",
    "            print( new_character, end='' )\n",
    "            if len(char_list) == MAXLEN :\n",
    "                char_list.pop(0)\n",
    "            char_list.append( new_character )\n",
    "        print()\n",
    "    except KeyError :\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563130d-178b-48c6-81f1-ba8b3185ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
