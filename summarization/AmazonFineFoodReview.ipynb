{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360640f7",
   "metadata": {
    "papermill": {
     "duration": 0.005804,
     "end_time": "2024-06-19T20:34:19.410479",
     "exception": false,
     "start_time": "2024-06-19T20:34:19.404675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summarization task - trained on Amazon Fine Food Review dataset\n",
    "\n",
    "Approach:\n",
    "1. Load and pre-process dataset:\n",
    "\n",
    "   Read the dataset using pandas, extract necessary columns, and handle missing values. Split the dataset into training, validation, and test sets.\n",
    "2. Tokenize and Vocabulary building:\n",
    "    \n",
    "    Use spaCy for tokenization and torchtext utilities to build vocabularies for the text and summary fields\n",
    "3. Create custom Dataset and Dataloader:\n",
    "    \n",
    "    Implement a custom dataset class to handle tokenization, vocabulary indexing, and padding. Use PyTorch's DataLoader to create iterable data loaders for training, validation, and testing.\n",
    "4. Define Model Architecture:\n",
    "    \n",
    "    Define the training loop, loss function, backpropagation, optimation steps. Implement evaluation functions\n",
    "5. Evaluate the Model:\n",
    "    Use of metrics such as ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc437f",
   "metadata": {
    "papermill": {
     "duration": 0.004743,
     "end_time": "2024-06-19T20:34:19.420752",
     "exception": false,
     "start_time": "2024-06-19T20:34:19.416009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare the dataset for training\n",
    "\n",
    "We will write custom code to handle tokenization, vocabulary building, and data loading:\n",
    "\n",
    "1. Load and preprocess the dataset\n",
    "2. Tokenize and build vocabulary\n",
    "3. Create a custom dataset and data loader\n",
    "\n",
    "### Step 1: Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97194e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:34:19.432606Z",
     "iopub.status.busy": "2024-06-19T20:34:19.432161Z",
     "iopub.status.idle": "2024-06-19T20:34:35.946516Z",
     "shell.execute_reply": "2024-06-19T20:34:35.945171Z"
    },
    "papermill": {
     "duration": 16.52355,
     "end_time": "2024-06-19T20:34:35.949298",
     "exception": false,
     "start_time": "2024-06-19T20:34:19.425748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9e46c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-19T20:34:35.961655Z",
     "iopub.status.busy": "2024-06-19T20:34:35.961219Z",
     "iopub.status.idle": "2024-06-19T20:34:45.151184Z",
     "shell.execute_reply": "2024-06-19T20:34:45.149941Z"
    },
    "papermill": {
     "duration": 9.199624,
     "end_time": "2024-06-19T20:34:45.154118",
     "exception": false,
     "start_time": "2024-06-19T20:34:35.954494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165fc06a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:34:45.166489Z",
     "iopub.status.busy": "2024-06-19T20:34:45.166064Z",
     "iopub.status.idle": "2024-06-19T20:34:45.453419Z",
     "shell.execute_reply": "2024-06-19T20:34:45.452183Z"
    },
    "papermill": {
     "duration": 0.297351,
     "end_time": "2024-06-19T20:34:45.456872",
     "exception": false,
     "start_time": "2024-06-19T20:34:45.159521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before dropping missing values: 568454\n",
      "Total samples after dropping missing values: 568427\n"
     ]
    }
   ],
   "source": [
    "# Display the number of rows in the dataset\n",
    "print(f\"Total samples before dropping missing values: {len(df)}\")\n",
    "\n",
    "# Extract necessary columns and drop missing values\n",
    "df = df[['Text', 'Summary']].dropna()\n",
    "\n",
    "print(f\"Total samples after dropping missing values: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f816bff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:34:45.474129Z",
     "iopub.status.busy": "2024-06-19T20:34:45.473565Z",
     "iopub.status.idle": "2024-06-19T20:34:46.982121Z",
     "shell.execute_reply": "2024-06-19T20:34:46.980936Z"
    },
    "papermill": {
     "duration": 1.520893,
     "end_time": "2024-06-19T20:34:46.984765",
     "exception": false,
     "start_time": "2024-06-19T20:34:45.463872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 454741\n",
      "Validation samples: 56843\n",
      "Test samples: 56843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset - 80/10/10\n",
    "# 80/20 split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 20 split as 10/10\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(valid_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac50e0",
   "metadata": {
    "papermill": {
     "duration": 0.0051,
     "end_time": "2024-06-19T20:34:46.995375",
     "exception": false,
     "start_time": "2024-06-19T20:34:46.990275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 2: Tokenize and Build Vocabulary\n",
    "\n",
    "We are using ```spaCy``` for tokenizing the text and summary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6041a9e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:34:47.008169Z",
     "iopub.status.busy": "2024-06-19T20:34:47.007715Z",
     "iopub.status.idle": "2024-06-19T20:38:40.338427Z",
     "shell.execute_reply": "2024-06-19T20:38:40.336917Z"
    },
    "papermill": {
     "duration": 233.340319,
     "end_time": "2024-06-19T20:38:40.341133",
     "exception": false,
     "start_time": "2024-06-19T20:34:47.000814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary for text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Text: 100%|██████████| 454741/454741 [03:25<00:00, 2217.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary for summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Summary: 100%|██████████| 454741/454741 [00:17<00:00, 26340.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocab size: 215083\n",
      "Summary vocab size: 48602\n"
     ]
    }
   ],
   "source": [
    "import spacy # used for tokenizing text\n",
    "import torch # we will use neural network from torch\n",
    "\n",
    "# torchtext: for processing text data and creating dataset and iterators\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize using spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def yield_tokens(data_iter, text_field):\n",
    "    for text in tqdm(data_iter[text_field], desc=f\"Tokenizing {text_field}\"):\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "# Build vocab for text and summary fields\n",
    "print(\"Building vocabulary for text...\")\n",
    "text_vocab = build_vocab_from_iterator(yield_tokens(train_df, 'Text'), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "print(\"Building vocabulary for summary...\")\n",
    "summary_vocab = build_vocab_from_iterator(yield_tokens(train_df, 'Summary'), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "\n",
    "# Set default index for unknown tokens\n",
    "text_vocab.set_default_index(text_vocab[\"<unk>\"])\n",
    "summary_vocab.set_default_index(summary_vocab[\"<unk>\"])\n",
    "\n",
    "print(f\"Text vocab size: {len(text_vocab)}\")\n",
    "print(f\"Summary vocab size: {len(summary_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fae2d",
   "metadata": {
    "papermill": {
     "duration": 0.207152,
     "end_time": "2024-06-19T20:38:40.753607",
     "exception": false,
     "start_time": "2024-06-19T20:38:40.546455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 3: Create custom Dataset and DataLoader\n",
    "\n",
    "Create a custom dataset class ```TextSummaryDataset``` that tokenizes input text and summaries, converts tokens to indices using the vocabularies, and pads sequences.\n",
    "\n",
    "```collate_batch``` does the crucial job for DataLoader by merging a list of samples into a single batch. It ensures that all sequences in a batch have the same length (which is the length of the longest sequence), an essential requirement for efficient computation on GPUs. It also converts lists of otkens into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc6cb32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:38:41.167079Z",
     "iopub.status.busy": "2024-06-19T20:38:41.166306Z",
     "iopub.status.idle": "2024-06-19T20:38:41.184208Z",
     "shell.execute_reply": "2024-06-19T20:38:41.182871Z"
    },
    "papermill": {
     "duration": 0.228078,
     "end_time": "2024-06-19T20:38:41.187077",
     "exception": false,
     "start_time": "2024-06-19T20:38:40.958999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self, df, text_vocab, summary_vocab, tokenizer):\n",
    "        self.df = df\n",
    "        self.text_vocab = text_vocab\n",
    "        self.summary_vocab = summary_vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['Text']\n",
    "        summary = self.df.iloc[idx]['Summary']\n",
    "        text_tokens = [self.text_vocab[\"<bos>\"]] + [self.text_vocab[token] for token in self.tokenizer(text)] + [self.text_vocab[\"<eos>\"]]\n",
    "        summary_tokens = [self.summary_vocab[\"<bos>\"]] + [self.summary_vocab[token] for token in self.tokenizer(summary)] + [self.summary_vocab[\"<eos>\"]]\n",
    "        return torch.tensor(text_tokens), torch.tensor(summary_tokens)\n",
    "\n",
    "train_dataset = TextSummaryDataset(train_df, text_vocab, summary_vocab, tokenizer)\n",
    "valid_dataset = TextSummaryDataset(valid_df, text_vocab, summary_vocab, tokenizer)\n",
    "test_dataset = TextSummaryDataset(test_df, text_vocab, summary_vocab, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, summary_list = [], []\n",
    "    for (_text, _summary) in batch:\n",
    "        text_list.append(torch.tensor(_text, dtype=torch.int64))\n",
    "        summary_list.append(torch.tensor(_summary, dtype=torch.int64))\n",
    "    text_batch = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=text_vocab[\"<pad>\"])\n",
    "    summary_batch = torch.nn.utils.rnn.pad_sequence(summary_list, padding_value=summary_vocab[\"<pad>\"])\n",
    "    return text_batch, summary_batch\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70a6cf",
   "metadata": {
    "papermill": {
     "duration": 0.205855,
     "end_time": "2024-06-19T20:38:41.595970",
     "exception": false,
     "start_time": "2024-06-19T20:38:41.390115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 4: Define the Model Architecture\n",
    "\n",
    "\n",
    "| Component | Role | Functionality |\n",
    "|-----------|------|---------------|\n",
    "| **Encoder** | Captures the context of the input sequence. | - Embeds the input tokens into dense vectors. <br> - Processes embeddings through a bidirectional GRU to capture forward and backward dependencies. <br> - Combines hidden states from both directions and projects them into the decoder's hidden state space. <br> - Returns the sequence of outputs from the GRU and the final hidden state. |\n",
    "| **Attention** | Helps the Decoder focus on different parts of the input sequence when generating each word of the summary. | - Calculates alignment scores between the current decoder hidden state and each encoder output. <br> - Normalizes alignment scores to obtain attention weights. <br> - Computes a weighted sum of the encoder outputs based on attention weights, producing a context vector that emphasizes relevant parts of the input. |\n",
    "| **Decoder** | Generates the output summary one token at a time, using the context provided by the attention mechanism. | - Embeds the previous output token (or the start token for the first step). <br> - Uses the context vector from the attention mechanism along with the embedded token to update its hidden state via a GRU. <br> - Projects the hidden state to the output vocabulary space to predict the next token. <br> - Combines the context vector, hidden state, and embedded token to produce the final output token probabilities. |\n",
    "| **Seq2Seq** | Orchestrates the overall encoding and decoding process. | - Initializes the Encoder and Decoder. <br> - Passes the input sequence through the Encoder to obtain encoder outputs and the final hidden state. <br> - Iteratively uses the Decoder to generate each token of the summary, applying the attention mechanism at each step. <br> - Handles teacher forcing during training and autoregressive generation during inference. |\n",
    "\n",
    "```\n",
    "Input Sequence --> [Encoder] --> Encoder Outputs + Final Hidden State\n",
    "                                      |\n",
    "                                      v\n",
    "                               [Attention]\n",
    "                                      |\n",
    "                                      v\n",
    "Previous Token + Context Vector --> [Decoder] --> Next Token\n",
    "                                      |\n",
    "                                      v\n",
    "                            (Repeat for next token)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Embedding Layer: \n",
    "The embedding layer (nn.Embedding) is a lookup table that maps each token index to a dense vector (embedding). This layer is trained to produce meaningful representations of tokens in a continuous vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed79401f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T20:38:42.011176Z",
     "iopub.status.busy": "2024-06-19T20:38:42.010713Z",
     "iopub.status.idle": "2024-06-19T20:38:42.025620Z",
     "shell.execute_reply": "2024-06-19T20:38:42.024261Z"
    },
    "papermill": {
     "duration": 0.226612,
     "end_time": "2024-06-19T20:38:42.028416",
     "exception": false,
     "start_time": "2024-06-19T20:38:41.801804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "    Encoder class is responsible for processing the input sequence \n",
    "    and capturing its contextual information through a sequence of operations\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hidden_dim, dropout):\n",
    "        \"\"\"\n",
    "            input_dim   : The size of the input vocab\n",
    "            emb_dim     : The dimensionality of the embedding vectors\n",
    "            enc_hid_dim : The dimensionality of the encoder hidden states\n",
    "            dec_hid_dim : The dimensionality of the decoder hidden states\n",
    "            droput      : The dropout rate to be applied to the embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer converts the input tokens into dense vectors of dim \"emb_dim\"\n",
    "        self.embedding = nn.Embedding(inpt_dim, emb_dim)\n",
    "        \n",
    "        # (bi-directional) GRU layer that processes the embedded input sequence\n",
    "        \"\"\"\n",
    "            emb_dim     : The dim of the input embeddings to the GRU\n",
    "            enc_hid_dim : The dim of hte GRU hidden states\n",
    "        \"\"\"\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        \n",
    "        \"\"\"\n",
    "            Fully connected layer - a linear layer that maps the concatenated hidden states \n",
    "            from the bidirectional GRU to the dim required by the decoder\n",
    "\n",
    "            enc_hid_dim * 2 : the concatenated size of the forward and backward hidden states\n",
    "                              from the bidirectional GRU.\n",
    "            dec_hid_dim     : the size required by the decoder hidden state.\n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        # Dropout layer applies dropout to the embeddings to prevent overfitting during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, src, src_len):\n",
    "            \n",
    "            # Encoding and Dropout:\n",
    "            # The input sequence src is first passed through the embedding layer to get dense vector representations.\n",
    "            # dropout is then applied to these embeddings\n",
    "            embedded = self.dropout(self.embedding(src))\n",
    "            \n",
    "            # Packing the Sequence:\n",
    "            # The embedded seqs are then packed into a packed sequenceb object, which helps the GRU handle\n",
    "            # variable-length seqs efficiently; The lenghts of the seq are moved to CPU to be used by the packing function.\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "            \n",
    "            # GRU Processing:\n",
    "            # The packed sequences are passed through the bidirectional GRU, \n",
    "            # giving the hidden states for each time in the packed sequence, \n",
    "            # and the final hidden seq for each direction of the GRU\n",
    "            packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "            \n",
    "            # Unpacking the Sequence:\n",
    "            # The packed seq is unpacked back to a packed seq of output\n",
    "            # outputs: are the hidden states at each time step, padded to the longest sequence.\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_sequence)\n",
    "            \n",
    "            # Concating hidden states: \n",
    "            # The final hidden states from both forward and backward GRU are concatenated\n",
    "            # and passed through a fully connected layer and a tanh function to create \n",
    "            # the initial state of the decoder\n",
    "            hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "            return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c2d5c",
   "metadata": {
    "papermill": {
     "duration": 0.205934,
     "end_time": "2024-06-19T20:38:42.438455",
     "exception": false,
     "start_time": "2024-06-19T20:38:42.232521",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 18,
     "sourceId": 2157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 268.340099,
   "end_time": "2024-06-19T20:38:44.573745",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-19T20:34:16.233646",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
